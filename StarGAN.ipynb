{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ccpy4OkFMEM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from pickle import load, dump\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FReLU(nn.Module):\n",
    "    def __init__(self, n_channel, kernel=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.funnel_condition = nn.Conv2d(n_channel, n_channel, kernel_size=kernel,stride=stride, padding=padding, groups=n_channel)\n",
    "        self.norm = nn.InstanceNorm2d(n_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tx = self.norm(self.funnel_condition(x))\n",
    "        out = torch.max(x, tx)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    @staticmethod\n",
    "    @torch.jit.script\n",
    "    def mish(x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return Mish.mish(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_nc, spectral_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pointwise Convolution\n",
    "        if spectral_norm:\n",
    "            self.query_conv = nn.utils.spectral_norm(nn.Conv2d(input_nc, input_nc // 8, kernel_size=1))\n",
    "            self.key_conv = nn.utils.spectral_norm(nn.Conv2d(input_nc, input_nc // 8, kernel_size=1))\n",
    "            self.value_conv = nn.utils.spectral_norm(nn.Conv2d(input_nc, input_nc, kernel_size=1))\n",
    "        else:\n",
    "            self.query_conv = nn.Conv2d(input_nc, input_nc // 8, kernel_size=1)\n",
    "            self.key_conv = nn.Conv2d(input_nc, input_nc // 8, kernel_size=1)\n",
    "            self.value_conv = nn.Conv2d(input_nc, input_nc, kernel_size=1)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        proj_query = self.query_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3]).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3])\n",
    "        s = torch.bmm(proj_query, proj_key) # バッチ毎の行列乗算\n",
    "        attention_map_T = self.softmax(s)\n",
    "        \n",
    "        proj_value = self.value_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3])\n",
    "        o = torch.bmm(proj_value, attention_map_T)\n",
    "        \n",
    "        o = o.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        out = x + self.gamma * o\n",
    "        \n",
    "        return out#, attention_map_T.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        #self.residual = nn.Sequential(\n",
    "        #    nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "        #    nn.BatchNorm2d(in_features),\n",
    "        #    nn.ReLU(inplace=True),\n",
    "        #    nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "        #    nn.BatchNorm2d(in_features)\n",
    "        #)\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        return F.relu(self.residual(x) + shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        squeeze = self.squeeze(x)\n",
    "        squeeze = squeeze.view(squeeze.size(0), -1)\n",
    "        excitation = self.excitation(squeeze).view(x.size(0), x.size(1), 1, 1)\n",
    "        return F.relu(x * excitation.expand_as(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSEBlock(nn.Module):\n",
    "    def __init__(self, in_features, reduction=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            FReLU(in_features),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features // reduction, in_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        residual = self.residual(x)\n",
    "        squeeze = self.squeeze(residual)\n",
    "        squeeze = squeeze.view(squeeze.size(0), -1)\n",
    "        excitation = self.excitation(squeeze).view(residual.size(0), residual.size(1), 1, 1)\n",
    "        return F.relu(residual * excitation.expand_as(residual) + shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d53nPWZFheB4"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, class_dim, n_residual_blocks=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        in_features = 64\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc + class_dim, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            FReLU(in_features)\n",
    "        ]\n",
    "        \n",
    "        # Downsampling\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                FReLU(out_features)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ ResidualSEBlock(in_features) ]\n",
    "        \n",
    "        model += [SelfAttention(in_features)]\n",
    "        \n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                FReLU(out_features)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "        \n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.Conv2d(in_features, output_nc, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0TccaIsSJxd"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, class_dim, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        n_features = 64\n",
    "        model = [\n",
    "            nn.utils.spectral_norm(nn.Conv2d(input_nc, n_features, kernel_size=4, stride=2, padding=1)),\n",
    "            Mish()\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.utils.spectral_norm(nn.Conv2d(n_features, n_features * 2, kernel_size=4, stride=2, padding=1)),\n",
    "            Mish()\n",
    "        ]\n",
    "        n_features *= 2\n",
    "        \n",
    "        model += [\n",
    "            nn.utils.spectral_norm(nn.Conv2d(n_features, n_features * 2, kernel_size=4, stride=2, padding=1)),\n",
    "            Mish()\n",
    "        ]\n",
    "        n_features *= 2\n",
    "        \n",
    "        model += [\n",
    "            nn.utils.spectral_norm(nn.Conv2d(n_features, n_features * 2, kernel_size=4, stride=2, padding=1)),\n",
    "            Mish()\n",
    "        ]\n",
    "        n_features *= 2\n",
    "        \n",
    "        model += [SelfAttention(n_features, spectral_norm=True)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        # For PatchGAN\n",
    "        self.conv_patch = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(n_features, 1, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        kernel_size = int(image_size / np.power(2, 4))\n",
    "        self.conv_class = nn.utils.spectral_norm(nn.Conv2d(n_features, class_dim, kernel_size=kernel_size, stride=1, padding=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  self.model(x)\n",
    "        \n",
    "        out = self.conv_patch(x)\n",
    "        \n",
    "        out_class = self.conv_class(x)\n",
    "        out_class = out_class.view(out_class.size(0), out_class.size(1))\n",
    "        \n",
    "        return out, out_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    @staticmethod\n",
    "    def loadImages(batch_size, folder_path, size):\n",
    "        imgs = ImageFolder(folder_path, transform=transforms.Compose([\n",
    "            transforms.Resize(int(size)),\n",
    "            transforms.RandomCrop(size),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            #transforms.RandomRotation(degrees=30),\n",
    "            #transforms.RandomPerspective(),\n",
    "            #transforms.ColorJitter(brightness=0, contrast=0.5, saturation=0.5),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "        return DataLoader(imgs, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=os.cpu_count(), pin_memory=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def showImage(image):\n",
    "        %matplotlib inline\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        PIL = transforms.ToPILImage()\n",
    "        \n",
    "        image = PIL(image)\n",
    "        fig = plt.figure(dpi=16)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, args):\n",
    "        has_cuda = torch.cuda.is_available() if not args.cpu else False\n",
    "        self.device = torch.device(\"cuda\" if has_cuda else \"cpu\")\n",
    "        self.dtype = torch.cuda.FloatTensor if has_cuda else torch.FloatTensor\n",
    "        self.itype = torch.cuda.LongTensor if has_cuda else torch.LongTensor\n",
    "        \n",
    "        self.args = args\n",
    "        self.num_channel = 3\n",
    "        \n",
    "        self.dataloader = Util.loadImages(self.args.batch_size, self.args.image_dir, self.args.image_size)\n",
    "        self.num_classes = len(os.listdir(self.args.image_dir))\n",
    "        \n",
    "        self.netG = Generator(self.num_channel, self.num_channel, class_dim=self.num_classes).to(self.device)\n",
    "        self.netD = Discriminator(self.num_channel, class_dim=self.num_classes, image_size=self.args.image_size).to(self.device)\n",
    "        self.state_loaded = False\n",
    "\n",
    "        self.netG.apply(self.weights_init)\n",
    "        self.netD.apply(self.weights_init)\n",
    "\n",
    "        self.optimizer_G = optim.Adam(self.netG.parameters(), lr=self.args.lr, betas=(0, 0.9))\n",
    "        self.optimizer_D = optim.Adam(self.netD.parameters(), lr=self.args.lr * self.args.mul_lr_dis, betas=(0, 0.9))\n",
    "        \n",
    "        self.pseudo_aug = 0.0\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Conv2d or type(module) == nn.ConvTranspose2d or type(module) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            module.bias.data.fill_(0)\n",
    "            \n",
    "    def save_state(self, num):\n",
    "        self.netG.cpu()\n",
    "        self.netD.cpu()\n",
    "        torch.save(self.netG.state_dict(), os.path.join(self.args.weight_dir, f'weight_G.{num}.pth'))\n",
    "        torch.save(self.netD.state_dict(), os.path.join(self.args.weight_dir, f'weight_D.{num}.pth'))\n",
    "        self.netG.to(self.device)\n",
    "        self.netD.to(self.device)\n",
    "            \n",
    "    def load_state(self):\n",
    "        if (os.path.exists('weight_G.pth') and os.path.exists('weight_D.pth')):\n",
    "            self.netG.load_state_dict(torch.load('weight_G.pth', map_location=self.device))\n",
    "            self.netD.load_state_dict(torch.load('weight_D.pth', map_location=self.device))\n",
    "            self.state_loaded = True\n",
    "            print('Loaded network state.')\n",
    "            \n",
    "    def save_resume(self):\n",
    "        with open(os.path.join('.', 'resume.pkl'), 'wb') as f:\n",
    "            dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(args, resume=True):\n",
    "        if resume and os.path.exists('resume.pkl'):\n",
    "            with open(os.path.join('.', 'resume.pkl'), 'rb') as f:\n",
    "                print('Load resume.')\n",
    "                solver = load(f)\n",
    "                solver.args = args\n",
    "                return solver\n",
    "        else:\n",
    "            return Solver(args)\n",
    "        \n",
    "    def trainGAN(self, epoch, iters, max_iters, real_img, real_label):\n",
    "        ### Train CycleGAN with WGAN-gp.\n",
    "        \n",
    "        loss = {}\n",
    "        L1_loss = nn.L1Loss()\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "        # Generate target domain labels randomly.\n",
    "        target_label = torch.LongTensor([random.randrange(self.num_classes) for _ in range(real_label.size(0))])\n",
    "        target_label = self.label2onehot(target_label, self.num_classes).to(self.device)\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                             Train the discriminator                              #\n",
    "        # ================================================================================ #\n",
    "\n",
    "        # Compute loss with real images.\n",
    "        real_src_score, real_cls_score = self.netD(real_img)\n",
    "        real_src_loss = - torch.mean(real_src_score)\n",
    "        real_cls_loss = BCE_loss(real_cls_score, real_label)\n",
    "        \n",
    "        # Compute loss with fake images.\n",
    "        fake_img = self.netG(real_img, target_label)\n",
    "        fake_src_score, _ = self.netD(fake_img)\n",
    "        \n",
    "        fake_src_loss = torch.mean(fake_src_score)\n",
    "        #p = random.uniform(0, 1)\n",
    "        #if 1 - self.pseudo_aug < p:\n",
    "        #    fake_src_loss = - torch.mean(fake_src_score)\n",
    "        #else:\n",
    "        #    fake_src_loss = torch.mean(fake_src_score)\n",
    "        #\n",
    "        ## Update Probability Augmentation.\n",
    "        #lz = (torch.sign(torch.logit(real_src_score)).mean()\n",
    "        #      - torch.sign(torch.logit(fake_src_score)).mean()) / 2\n",
    "        #if lz > self.args.aug_threshold:\n",
    "        #    self.pseudo_aug += self.args.aug_increment\n",
    "        #else:\n",
    "        #    self.pseudo_aug -= self.args.aug_increment\n",
    "        #self.pseudo_aug = min(1, max(0, self.pseudo_aug))\n",
    "        \n",
    "        # Total loss.\n",
    "        d_loss = real_src_loss + fake_src_loss + self.args.lambda_cls * real_cls_loss\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        self.optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_D.step()\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                               Train the generator                                #\n",
    "        # ================================================================================ #\n",
    "        \n",
    "        # Compute loss with reconstruction loss.\n",
    "        fake_img = self.netG(real_img, target_label)\n",
    "        recon_img = self.netG(fake_img, real_label)\n",
    "\n",
    "        fake_src_score, fake_cls_score = self.netD(fake_img)\n",
    "        fake_src_loss = - torch.mean(fake_src_score)\n",
    "        fake_cls_loss = BCE_loss(fake_cls_score, target_label)\n",
    "        reconst_loss = L1_loss(recon_img, real_img)\n",
    "\n",
    "        # Total loss.\n",
    "        g_loss = fake_src_loss + self.args.lambda_cls * fake_cls_loss + self.args.lambda_recon * reconst_loss\n",
    "\n",
    "        # Backward and optimize.\n",
    "        self.optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        # Logging.\n",
    "        loss['G/loss'] = g_loss.item()\n",
    "        loss['G/fake_cls_loss'] = fake_cls_loss.item()\n",
    "        loss['G/reconst_loss'] = reconst_loss.item()\n",
    "              \n",
    "        # Logging.\n",
    "        loss['D/loss'] = d_loss.item()\n",
    "        loss['D/real_cls_loss'] = real_cls_loss.item()\n",
    "        #loss['Augment/prob'] = self.pseudo_aug\n",
    "    \n",
    "        # Save\n",
    "        if iters == max_iters:\n",
    "            self.save_state(epoch)\n",
    "            img_name = str(epoch) + '_' + str(iters) + '.png'\n",
    "            img_path = os.path.join(self.args.result_dir, img_name)\n",
    "            self.save_sample(real_img, fake_img, img_path)\n",
    "        \n",
    "        return loss\n",
    "            \n",
    "    def label2onehot(self, labels, dim):\n",
    "        batch_size = labels.size(0)\n",
    "        out = torch.zeros(batch_size, dim)\n",
    "        out[np.arange(batch_size), labels.long()] = 1\n",
    "        return out\n",
    "    \n",
    "    def train(self, resume=True):\n",
    "        print(f'Use Device: {self.device}')\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        self.netG.train()\n",
    "        self.netD.train()\n",
    "        \n",
    "        hyper_params = {}\n",
    "        hyper_params['Image Dir'] = self.args.image_dir\n",
    "        hyper_params['Image Size'] = self.args.image_size\n",
    "        hyper_params['Result Dir'] = self.args.result_dir\n",
    "        hyper_params['Wieght Dir'] = self.args.weight_dir\n",
    "        hyper_params['Learning Rate'] = self.args.lr\n",
    "        hyper_params[\"Mul Discriminator's LR\"] = self.args.mul_lr_dis\n",
    "        hyper_params['Epochs'] = self.args.num_epochs\n",
    "        hyper_params['Batch Size'] = self.args.batch_size\n",
    "        hyper_params['lambda_cls'] = self.args.lambda_cls\n",
    "        hyper_params['lambda_recon'] = self.args.lambda_recon\n",
    "        #hyper_params['Probability Aug-Threshold'] = self.args.aug_threshold\n",
    "        #hyper_params['Probability Aug-Increment'] = self.args.aug_increment\n",
    "\n",
    "        for key in hyper_params.keys():\n",
    "            print(f'{key}: {hyper_params[key]}')\n",
    "        #experiment.log_parameters(hyper_params)\n",
    "        \n",
    "        max_iters = len(iter(self.dataloader))\n",
    "        \n",
    "        for epoch in range(1, self.args.num_epochs + 1):\n",
    "            if epoch < self.epoch:\n",
    "                continue\n",
    "            self.epoch = epoch + 1\n",
    "            \n",
    "            epoch_loss_G = 0.0\n",
    "            epoch_loss_D = 0.0\n",
    "            \n",
    "            for iters, (data, label) in enumerate(tqdm(self.dataloader)):\n",
    "                iters += 1\n",
    "                \n",
    "                data = data.to(self.device, non_blocking=True)\n",
    "                label = self.label2onehot(label, self.num_classes).to(self.device, non_blocking=True)\n",
    "                \n",
    "                loss = self.trainGAN(epoch, iters, max_iters, data, label)\n",
    "                epoch_loss_D += loss['D/loss']\n",
    "                epoch_loss_G += loss['G/loss'] if 'G/loss' in loss else 0\n",
    "                #experiment.log_metrics(loss)\n",
    "                    \n",
    "            print(f'{epoch} / {self.args.num_epochs}: Loss_G {epoch_loss_G}, Loss_D {epoch_loss_D}')\n",
    "            \n",
    "            if resume:\n",
    "                self.save_resume()\n",
    "              \n",
    "    def save_sample(self, real_img, fake_img, img_path):\n",
    "        N = real_img.size(0)\n",
    "        img = torch.cat((real_img.data, fake_img.data), dim=0)\n",
    "        save_image(img, img_path, nrow=N)\n",
    "        \n",
    "        #Util.showImage(real_img[0])\n",
    "        #Util.showImage(fake_img[0])\n",
    "    \n",
    "    def generate(self, num):\n",
    "        self.netG.eval()\n",
    "\n",
    "        for _ in range(num):\n",
    "            data, label = next(iter(self.dataloader))\n",
    "            real_img = data[0]\n",
    "            real_label = label[0]\n",
    "            real_img = real_img.unsqueeze(0).to(self.device)\n",
    "            \n",
    "            for i in range(self.num_classes):\n",
    "                label = i\n",
    "                onehot_label = self.label2onehot(torch.full((1,), label, dtype=torch.long), self.num_classes).to(self.device)\n",
    "                fake_img = self.netG(real_img, onehot_label).data\n",
    "                self.save_sample(real_img, fake_img, os.path.join(self.args.result_dir, f'generated_{real_label}-to-{i}_{time.time()}.png'))\n",
    "                \n",
    "        print('New picture was generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    solver = Solver.load(args, resume=not args.noresume)\n",
    "    solver.load_state()\n",
    "        \n",
    "    if args.generate > 0:\n",
    "        solver.generate(args.generate)\n",
    "        exit()\n",
    "    \n",
    "    solver.train(not args.noresume)\n",
    "    #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image_dir', type=str, default='')\n",
    "    parser.add_argument('--image_size', type=int, default=32)\n",
    "    parser.add_argument('--result_dir', type=str, default='results')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--mul_lr_dis', type=float, default=4)\n",
    "    parser.add_argument('--num_epochs', type=int, default=100)\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--lambda_cls', type=float, default=1)\n",
    "    parser.add_argument('--lambda_recon', type=float, default=10)\n",
    "    #parser.add_argument('--aug_threshold', type=float, default=0.6)\n",
    "    #parser.add_argument('--aug_increment', type=float, default=0.01)\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    parser.add_argument('--generate', type=int, default=0)\n",
    "    parser.add_argument('--noresume', action='store_true')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.result_dir):\n",
    "        os.mkdir(args.result_dir)\n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 667852,
     "sourceId": 1176357,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
